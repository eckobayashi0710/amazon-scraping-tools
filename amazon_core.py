# amazon_core.py
# Amazonå•†å“ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã¨Google Sheetsæ“ä½œã®ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ (æœ€çµ‚æ”¹å–„ç‰ˆ)

import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import requests
from bs4 import BeautifulSoup
import time
import re

class AmazonScraper:
    """
    Amazonå•†å“ãƒšãƒ¼ã‚¸ã®æƒ…å ±ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€Google Sheetsã‚’æ›´æ–°ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦é›»æ°—è£½å“ã¨æ›¸ç±ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚’åˆ‡ã‚Šæ›¿ãˆã‚‹ã€‚
    """
    
    def __init__(self, config, logger_callback=print):
        self.config = config
        self.logger = logger_callback
        self.mode = config.get('scrape_mode', 'é›»æ°—è£½å“') # GUIã‹ã‚‰ãƒ¢ãƒ¼ãƒ‰ã‚’å—ã‘å–ã‚‹
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        })

        # --- ãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ãŸãƒ˜ãƒƒãƒ€ãƒ¼ã‚’å®šç¾© ---
        self.ELECTRONICS_HEADERS = [
            'ASIN', 'å•†å“å', 'ãƒ–ãƒ©ãƒ³ãƒ‰', 'ä¾¡æ ¼', 'å•†å“ã®ç‰¹å¾´', 'ãƒ¡ãƒ¼ã‚«ãƒ¼', 'ãƒ¡ãƒ¼ã‚«ãƒ¼å‹ç•ª', 'è£½å“å‹ç•ª', 'å•†å“ãƒ¢ãƒ‡ãƒ«ç•ªå·',
            'è‰²', 'ã‚«ãƒ©ãƒ¼', 'ã‚µã‚¤ã‚º', 'è£½å“ã‚µã‚¤ã‚º', 'æ¢±åŒ…ã‚µã‚¤ã‚º', 'å•†å“ã®é‡é‡', 'å•†å“é‡é‡', 'æè³ª', 'ç´ æ',
            'æ¥ç¶šæŠ€è¡“', 'ãƒ˜ãƒƒãƒ‰ãƒ•ã‚©ãƒ³ã‚¸ãƒ£ãƒƒã‚¯', 'ã‚±ãƒ¼ãƒ–ãƒ«ã®ç‰¹å¾´', 'ã‚±ãƒ¼ãƒ–ãƒ«ã®å½¢çŠ¶', 'ã‚³ãƒã‚¯ã‚¿ã®ã‚ªã‚¹/ãƒ¡ã‚¹',
            'ä»˜å±å“', 'ä»˜å±ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆ', 'é›»æ± ä½¿ç”¨', 'é›»æ± ä»˜å±', 'ä¿è¨¼ã«ã¤ã„ã¦', 'ãƒ¡ãƒ¼ã‚«ãƒ¼ã«ã‚ˆã‚Šè£½é€ ä¸­æ­¢ã«ãªã‚Šã¾ã—ãŸ',
            'OS', 'è£½å“ã®ç‰¹å¾´', 'ãƒ¬ãƒ¼ã‚¶ãƒ¼ã‚¯ãƒ©ã‚¹', 'å¯¾è±¡å¹´é½¢', 'åˆ¶å¾¡ã‚¿ã‚¤ãƒ—', 'å•†å“ç”¨é€”ãƒ»ä½¿ç”¨æ–¹æ³•', 'ãƒ¦ãƒ‹ãƒƒãƒˆæ•°', 'ã‚¹ã‚¿ã‚¤ãƒ«',
            'é›»æ± ', 'ç™ºå£²æ—¥', 'Amazon.co.jp ã§ã®å–ã‚Šæ‰±ã„é–‹å§‹æ—¥', 'Amazon å£²ã‚Œç­‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼è©•ä¾¡', 'ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°'
        ]
        
        self.BOOK_HEADERS = [
            'ASIN', 'å•†å“å', 'è‘—è€…', 'ä¾¡æ ¼', 'å‡ºç‰ˆç¤¾', 'ç™ºå£²æ—¥', 'è¨€èª', 
            'ãƒšãƒ¼ã‚¸æ•°', 'å˜è¡Œæœ¬', 'æ–‡åº«', 'ãƒšãƒ¼ãƒ‘ãƒ¼ãƒãƒƒã‚¯', # ãƒšãƒ¼ã‚¸æ•°é–¢é€£
            'ISBN-10', 'ISBN-13', 'å¯¸æ³•', 'Amazon å£²ã‚Œç­‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°', 'ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼è©•ä¾¡', 'ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°'
        ]

        # å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ã«å¿œã˜ã¦ä½¿ç”¨ã™ã‚‹ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’æ±ºå®š
        if self.mode == 'æ›¸ç±':
            self.ALL_HEADERS = self.BOOK_HEADERS
            self.logger("ğŸ“– æ›¸ç±ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")
        else:
            self.ALL_HEADERS = self.ELECTRONICS_HEADERS
            self.logger("ğŸ”Œ é›»æ°—è£½å“ãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œã—ã¾ã™ã€‚")

        self._setup_gsheets()

    def _setup_gsheets(self):
        self.logger("ğŸ”§ Google Sheets ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ä¸­...")
        try:
            scopes = ["https://www.googleapis.com/auth/spreadsheets"]
            creds = Credentials.from_service_account_file(self.config["json_path"], scopes=scopes)
            self.sheets_service = build("sheets", "v4", credentials=creds, cache_discovery=False)
            gc = gspread.authorize(creds)
            spreadsheet = gc.open_by_key(self.config["spreadsheet_id"])
            self.sheet = spreadsheet.worksheet(self.config["sheet_name"])
            self.logger("âœ… Google Sheets ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†")
        except gspread.exceptions.WorksheetNotFound:
            self.logger(f"âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{self.config['sheet_name']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            raise
        except Exception as e:
            self.logger(f"âŒ Google Sheetsã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise

    def _column_letter_to_number(self, column_letter):
        return gspread.utils.a1_to_rowcol(f"{column_letter}1")[1]

    def _clean_key(self, text):
        """ã‚­ãƒ¼ã‹ã‚‰ã‚³ãƒ­ãƒ³ã€ç©ºç™½ã€ç›®ã«è¦‹ãˆãªã„åˆ¶å¾¡æ–‡å­—ãªã©ã‚’é™¤å»ã™ã‚‹"""
        if not text:
            return ""
        return re.sub(r'[\s:â€â€]', '', text)

    def _parse_details_table(self, table, data):
        """è©³ç´°æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«(tr/th/td)ã‚’è§£æã—ã¦dataè¾æ›¸ã‚’æ›´æ–°ã™ã‚‹"""
        if not table:
            return
        for tr in table.find_all('tr'):
            th = tr.find(['th', 'td'], class_='prodDetSectionEntry')
            td = tr.find('td', class_='prodDetInfoEntry')
            if not th or not td:
                th = tr.find('th')
                td = tr.find('td')
            
            if th and td:
                key = self._clean_key(th.text)
                value = re.sub(r'\s+', ' ', td.text).strip()
                if key and key not in data:
                    data[key] = value

    def _get_authors(self, soup, data_from_table):
        """è¤‡æ•°ã®å ´æ‰€ã‹ã‚‰è‘—è€…æƒ…å ±ã‚’æ¢ã—ã€æœ€åˆã«è¦‹ã¤ã‹ã£ãŸã‚‚ã®ã‚’è¿”ã™"""
        # 1. å•†å“åä¸‹ã® bylineInfo ã‹ã‚‰å–å¾— (æœ€å„ªå…ˆ)
        byline_tag = soup.find('div', id='bylineInfo_feature_div')
        if byline_tag:
            author_links = byline_tag.select('.author .a-link-normal')
            if author_links:
                authors = list(dict.fromkeys([a.text.strip() for a in author_links]))
                self.logger("  - è‘—è€…æƒ…å ±ã‚’ bylineInfo ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸã€‚")
                return ", ".join(authors)

        # 2. ã€Œè‘—è€…ã«ã¤ã„ã¦ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—
        author_bio_div = soup.find('div', class_='a-row a-spacing-small about-author-container')
        if author_bio_div:
            name_tag = author_bio_div.find('a', class_='a-link-normal')
            if name_tag:
                self.logger("  - è‘—è€…æƒ…å ±ã‚’ã€Œè‘—è€…ã«ã¤ã„ã¦ã€ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸã€‚")
                return name_tag.text.strip()

        # 3. è©³ç´°æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—
        if data_from_table.get('è‘—è€…'):
            self.logger("  - è‘—è€…æƒ…å ±ã‚’è©³ç´°æƒ…å ±ãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰å–å¾—ã—ã¾ã—ãŸã€‚")
            return data_from_table.get('è‘—è€…')
            
        return ""

    def _scrape_product_data(self, url):
        self.logger(f"ğŸ” URLã‚’è§£æä¸­: {url[:60]}...")
        max_retries = self.config.get('max_retries', 3)
        retry_delay = self.config.get('retry_delay', 5.0)

        for attempt in range(max_retries + 1):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                soup = BeautifulSoup(response.text, 'html.parser')
                
                data = {}

                # â–¼â–¼â–¼ æ”¹å–„ç‚¹: ASINæŠ½å‡ºã®æ­£è¦è¡¨ç¾ã‚’ä¿®æ­£ â–¼â–¼â–¼
                asin_match = re.search(r'/(dp|gp/product)/([A-Z0-9]{10})', url)
                data['ASIN'] = asin_match.group(2) if asin_match else 'N/A'

                # --- å…±é€šæƒ…å ± ---
                # â–¼â–¼â–¼ æ”¹å–„ç‚¹: å•†å“åã‚’ã‚ˆã‚Šå …ç‰¢ã«å–å¾— â–¼â–¼â–¼
                title_tag = soup.find('span', id='productTitle')
                if title_tag:
                    full_title = title_tag.text.strip()
                    cleaned_title = re.sub(r'\s*\(.+?\)$|\s*\[.+?\]$', '', full_title).strip()
                    data['å•†å“å'] = cleaned_title
                else:
                    data['å•†å“å'] = ''
                
                price_whole = soup.select_one('.a-price-whole')
                data['ä¾¡æ ¼'] = price_whole.text.strip().replace(',', '') if price_whole else ''
                
                feature_ul = soup.find('ul', class_='a-unordered-list a-vertical a-spacing-mini')
                if feature_ul:
                    features = [li.text.strip() for li in feature_ul.find_all('li')]
                    data['å•†å“ã®ç‰¹å¾´'] = "\n".join(features)

                # --- è©³ç´°æƒ…å ±ã®å–å¾— ---
                details_section_list = soup.find('div', id='detailBullets_feature_div')
                if details_section_list:
                    self.logger("  - è©³ç´°æƒ…å ±(detailBullets List)ã‚’è§£æä¸­...")
                    for li in details_section_list.select('li'):
                        key_tag = li.select_one('span.a-text-bold')
                        if not key_tag: continue
                        key = self._clean_key(key_tag.text)
                        
                        if "Amazonå£²ã‚Œç­‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°" in key:
                            value = li.text.replace(key_tag.text, '').strip()
                            data['Amazon å£²ã‚Œç­‹ãƒ©ãƒ³ã‚­ãƒ³ã‚°'] = re.sub(r'\s+', ' ', value)
                        elif "ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼" in key:
                            rating_span = li.select_one('span.a-icon-alt')
                            data['ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼è©•ä¾¡'] = rating_span.text.strip() if rating_span else ''
                            count_span = li.select_one('#acrCustomerReviewText')
                            data['ã‚«ã‚¹ã‚¿ãƒãƒ¼ãƒ¬ãƒ“ãƒ¥ãƒ¼æ•°'] = count_span.text.strip() if count_span else ''
                        else:
                            value_span = key_tag.find_next_sibling('span')
                            if value_span and key not in data:
                                data[key] = value_span.text.strip()
                
                details_table_book = soup.find('table', id='productDetails_detailBullets_sections1')
                if details_table_book:
                    self.logger("  - è©³ç´°æƒ…å ±(Book Table)ã‚’è§£æä¸­...")
                    self._parse_details_table(details_table_book, data)

                details_table_tech = soup.find('table', id='productDetails_techSpec_section_1')
                if details_table_tech:
                    self.logger("  - è©³ç´°æƒ…å ±(Tech Spec Table)ã‚’è§£æä¸­...")
                    self._parse_details_table(details_table_tech, data)
                
                # --- ãƒ¢ãƒ¼ãƒ‰åˆ¥æƒ…å ±ã®å–å¾— ---
                if self.mode == 'æ›¸ç±':
                    data['è‘—è€…'] = self._get_authors(soup, data)
                    page_info_found = False
                    page_keys = ['å˜è¡Œæœ¬', 'æ–‡åº«', 'ãƒšãƒ¼ãƒ‘ãƒ¼ãƒãƒƒã‚¯', 'å˜è¡Œæœ¬ï¼ˆã‚½ãƒ•ãƒˆã‚«ãƒãƒ¼ï¼‰', 'å¤§å‹æœ¬']
                    for p_key in page_keys:
                        if p_key in data and 'ãƒšãƒ¼ã‚¸' in data[p_key]:
                            data['ãƒšãƒ¼ã‚¸æ•°'] = data[p_key]
                            page_info_found = True
                            break
                    if not page_info_found:
                        for key, value in data.items():
                            if 'ãƒšãƒ¼ã‚¸' in str(value):
                                data['ãƒšãƒ¼ã‚¸æ•°'] = value
                                break
                else: # é›»æ°—è£½å“ãƒ¢ãƒ¼ãƒ‰
                    byline_tag = soup.find('div', id='bylineInfo_feature_div')
                    if byline_tag:
                        data['ãƒ–ãƒ©ãƒ³ãƒ‰'] = byline_tag.text.strip()

                self.logger("âœ… è§£ææˆåŠŸã€‚")
                return data

            except requests.exceptions.RequestException as e:
                self.logger(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {e}")
                if attempt < max_retries:
                    wait_time = retry_delay * (2 ** attempt)
                    self.logger(f"ğŸ”„ {wait_time:.1f}ç§’å¾…æ©Ÿã—ã¦å†è©¦è¡Œã—ã¾ã™...")
                    time.sleep(wait_time)
                else:
                    self.logger("âŒ æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    return None
        return None

    def _check_and_create_headers(self):
        self.logger("ğŸ” ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèª...")
        try:
            start_col_letter = self.config['output_start_col_letter'].upper()
            
            current_headers = self.sheet.row_values(1)
            start_col_num = self._column_letter_to_number(start_col_letter)

            if len(current_headers) < start_col_num + len(self.ALL_HEADERS):
                padding = (start_col_num + len(self.ALL_HEADERS)) - len(current_headers)
                current_headers.extend([""] * padding)
            
            actual_headers = current_headers[start_col_num - 1 : start_col_num - 1 + len(self.ALL_HEADERS)]

            if actual_headers != self.ALL_HEADERS:
                self.logger(f"â„¹ï¸ '{self.mode}' ãƒ¢ãƒ¼ãƒ‰ã®ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆã¾ãŸã¯æ›´æ–°ã—ã¾ã™...")
                self.sheet.update(f"{start_col_letter}1", [self.ALL_HEADERS], value_input_option='USER_ENTERED')
                self.logger("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½œæˆ/æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                self.logger("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
        except Exception as e:
            self.logger(f"âš ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèªãƒ»ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

    def run_process(self):
        try:
            self.logger(f"\nğŸš€ Amazonå•†å“æƒ…å ±ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†é–‹å§‹ ({self.mode}ãƒ¢ãƒ¼ãƒ‰) ğŸš€")
            self._check_and_create_headers()
        except Exception as e:
            self.logger(f"CRITICAL: ãƒ˜ãƒƒãƒ€ãƒ¼ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚: {e}")
            return

        current_row = self.config["start_row"]
        url_col_num = self._column_letter_to_number(self.config["url_col_letter"])
        output_start_col_letter = self.config["output_start_col_letter"]
        consecutive_empty_count = 0

        while True:
            self.logger(f"\n--- {current_row}è¡Œç›®ã®å‡¦ç†ã‚’é–‹å§‹ ---")
            try:
                row_values = self.sheet.row_values(current_row)
                url = row_values[url_col_num - 1] if len(row_values) >= url_col_num else None
            except gspread.exceptions.APIError as e:
                 if e.response.status_code == 400:
                    self.logger("â„¹ï¸ ã“ã‚Œä»¥ä¸Šå‡¦ç†ã™ã‚‹è¡ŒãŒã‚ã‚Šã¾ã›ã‚“ã€‚å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                 else:
                    self.logger(f"âŒ Google APIã‚¨ãƒ©ãƒ¼: {e}")
                    break
            except Exception as e:
                self.logger(f"âŒ {current_row}è¡Œç›®ã®ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
                break

            if not url or not url.strip():
                consecutive_empty_count += 1
                self.logger(f"â„¹ï¸ URLãŒç©ºã§ã™ã€‚({consecutive_empty_count}/10)")
                if consecutive_empty_count >= 10:
                    self.logger("ğŸ›‘ 10ä»¶é€£ç¶šã§URLãŒç©ºã®ãŸã‚ã€å‡¦ç†ã‚’çµ‚äº†ã—ã¾ã™ã€‚")
                    break
                current_row += 1
                continue
            
            consecutive_empty_count = 0
            
            scraped_data = self._scrape_product_data(url)
            
            if scraped_data:
                self.logger(f"ğŸ“ {current_row}è¡Œç›®ã«å–å¾—ã—ãŸæƒ…å ±ã‚’æ›¸ãè¾¼ã¿ã¾ã™...")
                values_to_write = [scraped_data.get(header, "") for header in self.ALL_HEADERS]
                
                self.sheet.update(f"{output_start_col_letter}{current_row}", [values_to_write], value_input_option='USER_ENTERED')
                self.logger("âœ… æ›¸ãè¾¼ã¿å®Œäº†ã€‚")
            else:
                self.logger(f"âš ï¸ {current_row}è¡Œç›®ã®URLã®æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

            time.sleep(self.config.get("delay", 3))
            current_row += 1
        
        self.logger("\nğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ ğŸ‰")
