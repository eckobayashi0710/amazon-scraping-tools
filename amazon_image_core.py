# amazon_image_core.py
# Amazonå•†å“ãƒšãƒ¼ã‚¸ã®ç”»åƒURLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€Google Sheetsã‚’æ›´æ–°ã™ã‚‹ã‚³ã‚¢ãƒ­ã‚¸ãƒƒã‚¯ (v6.1 - æ­£è¦è¡¨ç¾ã‚’è²ªæ¬²ãƒãƒƒãƒã«å¤‰æ›´)

import gspread
from google.oauth2.service_account import Credentials
from googleapiclient.discovery import build
import requests
from bs4 import BeautifulSoup
import time
import re
import json

class AmazonImageScraper:
    """
    Amazonå•†å“ãƒšãƒ¼ã‚¸ã®ç”»åƒURLã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã€Google Sheetsã‚’æ›´æ–°ã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚
    """
    
    def __init__(self, config, logger_callback=print):
        """
        ã‚³ãƒ³ã‚¹ãƒˆãƒ©ã‚¯ã‚¿ã€‚è¨­å®šæƒ…å ±ã¨ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–ã—ã¾ã™ã€‚
        """
        self.config = config
        self.logger = logger_callback
        self.session = requests.Session()
        # ãƒ–ãƒ©ã‚¦ã‚¶ã‚’å½è£…ã™ã‚‹ãŸã‚ã®ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/108.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
            "Accept-Language": "ja,en-US;q=0.9,en;q=0.8",
        })
        self._setup_gsheets()

    def _setup_gsheets(self):
        """
        Google Sheets APIã¸ã®æ¥ç¶šã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã—ã¾ã™ã€‚
        """
        self.logger("ğŸ”§ Google Sheets ã‚µãƒ¼ãƒ“ã‚¹ã‚’åˆæœŸåŒ–ä¸­...")
        try:
            scopes = ["https://www.googleapis.com/auth/spreadsheets"]
            creds = Credentials.from_service_account_file(self.config["json_path"], scopes=scopes)
            self.sheets_service = build("sheets", "v4", credentials=creds, cache_discovery=False)
            gc = gspread.authorize(creds)
            spreadsheet = gc.open_by_key(self.config["spreadsheet_id"])
            self.sheet = spreadsheet.worksheet(self.config["sheet_name"])
            self.logger("âœ… Google Sheets ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†")
        except gspread.exceptions.WorksheetNotFound:
            self.logger(f"âŒ ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆ '{self.config['sheet_name']}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            raise
        except Exception as e:
            self.logger(f"âŒ Google Sheetsã®èªè¨¼ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")
            raise

    def _column_letter_to_number(self, column_letter):
        """
        åˆ—ã®ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆï¼ˆ'A', 'B'ãªã©ï¼‰ã‚’æ•°å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚
        """
        return gspread.utils.a1_to_rowcol(f"{column_letter}1")[1]

    def _get_image_id(self, url):
        """
        ç”»åƒURLã‹ã‚‰ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªIDã‚’æŠ½å‡ºã—ã¾ã™ã€‚
        ä¾‹: /I/xxxxxxxx.jpg -> xxxxxxxx
        """
        if not isinstance(url, str):
            return None
        match = re.search(r'/I/([a-zA-Z0-9\-_+]+)\.', url)
        return match.group(1) if match else None

    def _scrape_image_urls(self, url):
        """
        æŒ‡å®šã•ã‚ŒãŸAmazonå•†å“ãƒšãƒ¼ã‚¸ã®URLã‹ã‚‰é«˜ç”»è³ªã®ç”»åƒURLã‚’æœ€å¤§10ä»¶ã¾ã§å–å¾—ã—ã¾ã™ã€‚
        â˜…â˜…â˜…ãƒ­ã‚¸ãƒƒã‚¯ã‚’åˆ·æ–°(v6.1)â˜…â˜…â˜…
        """
        self.logger(f"ğŸ–¼ï¸ ç”»åƒURLã‚’è§£æä¸­: {url[:60]}...")
        max_retries = self.config.get('max_retries', 3)
        retry_delay = self.config.get('retry_delay', 5.0)

        for attempt in range(max_retries + 1):
            try:
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                html_content = response.text
                
                ordered_urls = []
                processed_ids = set()

                # --- ã€ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯: scriptã‚¿ã‚°å†…ã®JSONãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥è§£æã€‘ ---
                self.logger("  - ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ (scriptå†…ã®JSONè§£æ) ã‚’è©¦è¡Œ...")
                
                # 'colorImages'ã®'initial'é…åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’æ­£è¦è¡¨ç¾ã§æŠ½å‡º
                # re.DOTALLã§æ”¹è¡Œã‚’ã¾ãŸã„ã æ¤œç´¢ã‚’å¯èƒ½ã«ã—ã€'.+'(è²ªæ¬²ãƒãƒƒãƒ)ã§é…åˆ—å…¨ä½“ã‚’ç¢ºå®Ÿã«å–å¾—ã™ã‚‹
                match = re.search(r"'colorImages'\s*:\s*{\s*'initial'\s*:\s*(\[.+\])\s*}", html_content, re.DOTALL)
                
                if match:
                    image_data_str = match.group(1)
                    # æŠ½å‡ºã—ãŸæ–‡å­—åˆ—ã‹ã‚‰ 'hiRes' ã®URLã‚’ã™ã¹ã¦è¦‹ã¤ã‘ã‚‹
                    urls_in_script = re.findall(r'"hiRes"\s*:\s*"(https?://[^"]+)"', image_data_str)
                    
                    for img_url in urls_in_script:
                        # URLã«å«ã¾ã‚Œã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ¦ãƒ‹ã‚³ãƒ¼ãƒ‰ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ (ä¾‹: \u002F -> /)
                        img_url = img_url.encode('utf-8').decode('unicode_escape')
                        
                        img_id = self._get_image_id(img_url)
                        if img_id and img_id not in processed_ids:
                            ordered_urls.append(img_url)
                            processed_ids.add(img_id)
                            self.logger(f"    - å–å¾—æˆåŠŸ: {img_url}")

                # --- ã€ä»£æ›¿ãƒ­ã‚¸ãƒƒã‚¯: ã‚µãƒ ãƒã‚¤ãƒ«ã‹ã‚‰å–å¾—ã€‘ ---
                if not ordered_urls:
                    self.logger("  - ä»£æ›¿ãƒ­ã‚¸ãƒƒã‚¯ ('altImages'ã®ã‚µãƒ ãƒã‚¤ãƒ«) ã‚’è©¦è¡Œ...")
                    soup = BeautifulSoup(html_content, 'html.parser')
                    alt_images_div = soup.find('div', id='altImages')
                    if alt_images_div:
                        thumbnails = alt_images_div.select('li.item.imageThumbnail img, li.item img')
                        for thumb in thumbnails:
                            src = thumb.get('src')
                            if src:
                                # ã‚µãƒ ãƒã‚¤ãƒ«URLã‚’é«˜è§£åƒåº¦URLã«å¤‰æ›
                                hires_url = re.sub(r'\._.*?_\.', '._SL1500_.', src)
                                thumb_id = self._get_image_id(hires_url)
                                if thumb_id and thumb_id not in processed_ids:
                                    ordered_urls.append(hires_url)
                                    processed_ids.add(thumb_id)
                                    self.logger(f"    - å–å¾—æˆåŠŸ (ä»£æ›¿): {hires_url}")

                if not ordered_urls:
                    self.logger("âš ï¸ ã“ã®ãƒšãƒ¼ã‚¸ã§ã¯ã©ã®æ–¹æ³•ã§ã‚‚ç”»åƒURLã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
                    return []

                self.logger(f"âœ… è§£æå®Œäº†ã€‚åˆè¨ˆ{len(ordered_urls)}ä»¶ã®ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªURLã‚’å–å¾—ã€‚")
                return ordered_urls[:10]

            except requests.exceptions.RequestException as e:
                self.logger(f"âŒ ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{max_retries + 1}): {e}")
                if attempt < max_retries:
                    time.sleep(retry_delay * (2 ** attempt))
                else:
                    self.logger("âŒ æœ€å¤§å†è©¦è¡Œå›æ•°ã«é”ã—ã¾ã—ãŸã€‚")
                    return None
        return None


    def _check_and_create_headers(self):
        """
        ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã®ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ç¢ºèªã—ã€å¿…è¦ã§ã‚ã‚Œã°ä½œæˆã—ã¾ã™ã€‚
        """
        self.logger("ğŸ” ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèª...")
        try:
            start_col_letter = self.config['output_start_col_letter'].upper()
            expected_headers = [f"ç”»åƒURL {i+1}" for i in range(10)]
            
            current_headers = self.sheet.row_values(1)
            start_col_num = self._column_letter_to_number(start_col_letter)

            if len(current_headers) < start_col_num + len(expected_headers):
                padding = (start_col_num + len(expected_headers)) - len(current_headers)
                current_headers.extend([""] * padding)
            
            actual_headers = current_headers[start_col_num - 1 : start_col_num - 1 + len(expected_headers)]

            if actual_headers != expected_headers:
                self.logger("â„¹ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’ä½œæˆã¾ãŸã¯æ›´æ–°ã—ã¾ã™...")
                self.sheet.update(f"{start_col_letter}1", [expected_headers], value_input_option='USER_ENTERED')
                self.logger("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã®ä½œæˆ/æ›´æ–°ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
            else:
                self.logger("âœ… ãƒ˜ãƒƒãƒ€ãƒ¼ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
        except Exception as e:
            self.logger(f"âš ï¸ ãƒ˜ãƒƒãƒ€ãƒ¼ã®ç¢ºèªãƒ»ä½œæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            raise

    def run_process(self):
        """
        ãƒ¡ã‚¤ãƒ³ã®å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰URLã‚’èª­ã¿å–ã‚Šã€ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ã¦çµæœã‚’æ›¸ãè¾¼ã¿ã¾ã™ã€‚
        """
        try:
            self.logger("\nğŸš€ Amazonå•†å“ç”»åƒã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å‡¦ç†é–‹å§‹ (ãƒãƒƒãƒãƒ¢ãƒ¼ãƒ‰) ğŸš€")
            self._check_and_create_headers()
        except Exception as e:
            self.logger(f"CRITICAL: ãƒ˜ãƒƒãƒ€ãƒ¼ã®æº–å‚™ã«å¤±æ•—ã—ãŸãŸã‚ã€å‡¦ç†ã‚’ä¸­æ­¢ã—ã¾ã™ã€‚: {e}")
            return

        start_row = self.config["start_row"]
        url_col_num = self._column_letter_to_number(self.config["url_col_letter"])
        output_start_col_letter = self.config["output_start_col_letter"]
        output_start_col_num = self._column_letter_to_number(output_start_col_letter)
        batch_size = self.config["batch_size"]
        
        current_row = start_row
        
        while True:
            self.logger(f"\n--- ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {current_row}è¡Œç›®ã‹ã‚‰{batch_size}è¡Œåˆ† ---")
            
            range_to_get = f"A{current_row}:Z{current_row + batch_size - 1}"
            try:
                self.logger(f"ğŸšš ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ä¸­... (ç¯„å›²: {range_to_get})")
                batch_data = self.sheet.get(range_to_get, major_dimension='ROWS', value_render_option='FORMATTED_VALUE')

            except gspread.exceptions.APIError as e:
                if e.response.status_code == 400:
                     self.logger(f"â„¹ï¸ æŒ‡å®šç¯„å›²({range_to_get})ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ã‚·ãƒ¼ãƒˆã®çµ‚ç«¯ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")
                else:
                    self.logger(f"âŒ APIã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—: {e}")
                break
            
            if not batch_data:
                self.logger("â„¹ï¸ ã“ã‚Œä»¥ä¸Šå‡¦ç†ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
                break

            update_payload = []
            
            for i, row_values in enumerate(batch_data):
                actual_row_num = current_row + i
                
                url = row_values[url_col_num - 1] if len(row_values) >= url_col_num else None
                
                if not url or not url.strip():
                    continue
                
                output_cell_value = row_values[output_start_col_num - 1] if len(row_values) >= output_start_col_num else None
                if output_cell_value and output_cell_value.strip():
                    self.logger(f"  - {actual_row_num}è¡Œç›®: å‡¦ç†æ¸ˆã¿ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—")
                    continue

                image_urls = self._scrape_image_urls(url)
                
                if image_urls is not None:
                    values_to_write = image_urls + [""] * (10 - len(image_urls))
                    update_payload.append({
                        'range': f"{output_start_col_letter}{actual_row_num}",
                        'values': [values_to_write]
                    })
                else:
                    self.logger(f"âš ï¸ {actual_row_num}è¡Œç›®ã®URLã®æƒ…å ±å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")

                time.sleep(self.config.get("delay", 3))

            if update_payload:
                self.logger(f"ğŸ’¾ {len(update_payload)}ä»¶ã®å‡¦ç†çµæœã‚’ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ä¸€æ‹¬æ›¸ãè¾¼ã¿ä¸­...")
                try:
                    self.sheet.batch_update(update_payload, value_input_option='USER_ENTERED')
                    self.logger("âœ… æ›¸ãè¾¼ã¿å®Œäº†ã€‚")
                except gspread.exceptions.APIError as e:
                    self.logger(f"âŒ APIã‚¨ãƒ©ãƒ¼ã«ã‚ˆã‚Šä¸€æ‹¬æ›¸ãè¾¼ã¿ã«å¤±æ•—: {e}")
            else:
                self.logger("â„¹ï¸ ã“ã®ãƒãƒƒãƒã§ã¯æ›¸ãè¾¼ã‚€ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")

            current_row += len(batch_data)
            
            if len(batch_data) < batch_size:
                self.logger("â„¹ï¸ ã‚·ãƒ¼ãƒˆã®æœ€çµ‚è¡Œã¾ã§å‡¦ç†ã—ã¾ã—ãŸã€‚")
                break

        self.logger("\nğŸ‰ å…¨å‡¦ç†å®Œäº†ï¼ ğŸ‰")
